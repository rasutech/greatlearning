{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: This is a sample solution for the project. Projects will NOT be graded on the basis of how well the submission matches this sample solution. Projects will be graded on the basis of the rubric only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background & Context\n",
    "\n",
    "There is a huge demand for used cars in the Indian Market today. As sales of new cars have slowed down in the recent past, the pre-owned car market has continued to grow over the past years and is larger than the new car market now. Cars4U is a budding tech start-up that aims to find footholes in this market.\n",
    "\n",
    "In 2018-19, while new car sales were recorded at 3.6 million units, around 4 million second-hand cars were bought and sold. There is a slowdown in new car sales and that could mean that the demand is shifting towards the pre-owned market. In fact, some car sellers replace their old cars with pre-owned cars instead of buying new ones. Unlike new cars, where price and supply are fairly deterministic and managed by OEMs (Original Equipment Manufacturer / except for dealership level discounts which come into play only in the last stage of the customer journey), used cars are very different beasts with huge uncertainty in both pricing and supply. Keeping this in mind, the pricing scheme of these used cars becomes important in order to grow in the market.\n",
    "\n",
    "As a senior data scientist at Cars4U, you have to come up with a pricing model that can effectively predict the price of used cars and can help the business in devising profitable strategies using differential pricing. For example, if the business knows the market price, it will never sell anything below it. \n",
    "\n",
    "# Objective\n",
    "\n",
    "* Explore and visualize the dataset.\n",
    "\n",
    "* Build a linear regression model to predict the prices of used cars.\n",
    "\n",
    "* Generate a set of insights and recommendations that will help the business.\n",
    "\n",
    "**Data Dictionary -** \n",
    "\n",
    "S.No. : Serial Number\n",
    "\n",
    "Name : Name of the car which includes Brand name and Model name\n",
    "\n",
    "Location : The location in which the car is being sold or is available for purchase Cities\n",
    "\n",
    "Year : Manufacturing year of the car\n",
    "\n",
    "Kilometers_driven : The total kilometers driven in the car by the previous owner(s) in KM.\n",
    "\n",
    "Fuel_Type : The type of fuel used by the car. (Petrol, Diesel, Electric, CNG, LPG)\n",
    "\n",
    "Transmission : The type of transmission used by the car. (Automatic / Manual)\n",
    "\n",
    "Owner : Type of ownership\n",
    "\n",
    "Mileage : The standard mileage offered by the car company in kmpl or km/kg\n",
    "\n",
    "Engine : The displacement volume of the engine in CC.\n",
    "\n",
    "Power : The maximum power of the engine in bhp.\n",
    "\n",
    "Seats : The number of seats in the car.\n",
    "\n",
    "New_Price : The price of a new car of the same model in INR Lakhs.(1 Lakh = 100, 000)\n",
    "\n",
    "Price : The price of the used car in INR Lakhs (1 Lakh = 100, 000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nb_black'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-623c7bdf3547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# this will help in making the Python code more structured automatically (good coding practice)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nb_black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2342\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2344\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2345\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-58>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nb_black'"
     ]
    }
   ],
   "source": [
    "# this will help in making the Python code more structured automatically (good coding practice)\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "# to do mathematical computations\n",
    "import math\n",
    "\n",
    "# To build linear model for prediction\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# To check model performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and exploring the data\n",
    "\n",
    "Loading the data into python to explore and understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"used_cars_data.csv\")\n",
    "print(f\"There are {data.shape[0]} rows and {data.shape[1]} columns.\")  # f-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a copy of the data\n",
    "df = data.copy()\n",
    "\n",
    "# let's view a sample of the data\n",
    "np.random.seed(1)  # to get the same random results every time\n",
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`S.No.` is just an index for the data entry. In all likelihood, this column will not be a significant factor in determining the price of the car. \n",
    "Having said that, there are instances where the index of the data entry contains the information about time factor (an entry with a smaller index corresponds to data entered years ago). Therefore, we will not drop this variable just yet. Let us see if there is any relationship with the price when we do a bivariate analysis.\n",
    "\n",
    "`Car names` contain a lot of model information. Let us check how many individual names we have. If they are too many, we can process this column to extract important information.\n",
    "\n",
    "`Mileage`, `Engine`, and `Power` will also need some processing before we are able to explore them. We'll have to extract numerical information from these columns.\n",
    "\n",
    "The `New_Price` column also needs some processing. This one also contains strings and a lot of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `Mileage`, `Engine`, `Power` and `New_Price` are objects when they should ideally be numerical. To be able to get summary statistics for these columns, We will have to process them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us process `Mileage`, `Engine`, `Power`, and `New_Price` columns to extract numerical values from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Mileage\n",
    "\n",
    "We have car mileage in two units as per the data dictionary - kmpl and km/kg.\n",
    "\n",
    "After a quick research on the internet, it is clear that these 2 units are used for cars of 2 different fuel types.\n",
    "\n",
    "* kmpl (kilometers per litre) is used for petrol and diesel cars.\n",
    "* km/kg (kilometers per kg) is used for CNG and LPG-based engines.\n",
    "\n",
    "We have the variable `Fuel_type` in our data. Let us check if these observations hold true in our data also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mileage = df[\"Mileage\"].str.split(\" \", expand=True)\n",
    "df_mileage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's verify that there are two units\n",
    "df_mileage[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create two new columns for mileage values and units\n",
    "df[\"km_per_unit_fuel\"] = df_mileage[0].astype(float)\n",
    "df[\"mileage_unit\"] = df_mileage[1]\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if the units correspond to the fuel types\n",
    "df.groupby(by=[\"Fuel_Type\", \"mileage_unit\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, km/kg is for CNG/LPG cars and kmpl is for Petrol and Diesel cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Engine \n",
    "\n",
    "As per the data dictionary, the `Engine` column indicates the displacement volume of the engine in CC.\n",
    "\n",
    "Let's extract the numerical part of the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engine = df[\"Engine\"].str.split(\" \", expand=True)\n",
    "df_engine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's verify that there is only one unit\n",
    "df_engine[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we will create a new column for engine values\n",
    "df[\"engine_num\"] = df_engine[0].astype(float)\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Power \n",
    "\n",
    "As per the data dictionary, the `Power` column indicates the maximum power of the engine in bhp.\n",
    "\n",
    "Let's extract the numerical part of the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power = df[\"Power\"].str.split(\" \", expand=True)\n",
    "df_power.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's verify that there is only one unit\n",
    "df_power[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a new column for power values\n",
    "df[\"power_num\"] = df_power[0].astype(float)\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error occurred as there are some non-numeric values like 'null' in the numeric part of the original `Power` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the values where there is 'null'\n",
    "df_power[df_power[0] == \"null\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a function to convert the Power column to float\n",
    "\n",
    "\n",
    "def power_to_num(power_val):\n",
    "    \"\"\"\n",
    "    This function takes in a string representing he maximum power of the engine in bhp\n",
    "    and converts it to a number. For example, '126.2 bhp' becomes 126.2\n",
    "    If the input is already numeric, which probably means it's NaN,\n",
    "    this function just returns np.nan.\n",
    "    \"\"\"\n",
    "    if isinstance(power_val, str):  # checks if `power_val` is a string\n",
    "        if power_val.startswith(\"null\"):  # checks if `power_val` starts with 'null'\n",
    "            return np.nan\n",
    "        elif power_val.endswith(\"bhp\"):\n",
    "            return float(power_val.replace(\" bhp\", \"\"))\n",
    "    else:  # this happens when the power is np.nan\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the function to the Power column\n",
    "df[\"power_num\"] = df[\"Power\"].apply(power_to_num)\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. New_Price \n",
    "\n",
    "We know that `New_Price` is the price of a new car of the same model in INR Lakhs (1 Lakh INR = 100, 000 INR)\n",
    "\n",
    "Let's extract the numerical part of the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_price = df[\"New_Price\"].str.split(\" \", expand=True)\n",
    "df_new_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's verify that there is only one unit\n",
    "df_new_price[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are two units - Lakh and Cr.\n",
    "* 1 Cr = 100 Lakhs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a function to convert the New_Price column to float\n",
    "\n",
    "\n",
    "def new_price_to_num(new_price_val):\n",
    "    \"\"\"\n",
    "    This function takes in a string representing a new car price\n",
    "    and converts it to a number. For example, '8.61 Lakh' becomes 8.61.\n",
    "    If the input is already numeric, which probably means it's NaN,\n",
    "    this function just returns np.nan.\n",
    "    \"\"\"\n",
    "    if isinstance(new_price_val, str):  # checks if `new_price_val` is a string\n",
    "        multiplier = 1  # handles Lakh vs Cr values\n",
    "        if new_price_val.endswith(\"Lakh\"):\n",
    "            multiplier = 1\n",
    "        elif new_price_val.endswith(\"Cr\"):\n",
    "            multiplier = 100\n",
    "        return float(new_price_val.replace(\" Lakh\", \"\").replace(\" Cr\", \"\")) * multiplier\n",
    "    else:  # this happens when the current new_price is np.nan\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the function to the New_Price column\n",
    "df[\"new_price_num\"] = df[\"New_Price\"].apply(new_price_to_num)\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Name` column in the current format might not be very useful in our analysis.\n",
    "Since the name contains both the brand name and the model name of the vehicle, the column would have too many unique values to be useful in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Name\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2041 unique names, car names are not going to be great predictors of the price in our current data.\n",
    "\n",
    "But we can process this column to extract important information and see if that reduces the number of levels for this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Car Brand Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract Brand Names\n",
    "df[\"Brand\"] = df[\"Name\"].apply(lambda x: x.split(\" \")[0].lower())\n",
    "\n",
    "# Check the data\n",
    "df[\"Brand\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.countplot(y=\"Brand\", data=df, order=df[\"Brand\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Car Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Model Names\n",
    "df[\"Model\"] = df[\"Name\"].apply(lambda x: x.split(\" \")[1].lower())\n",
    "\n",
    "# Check the data\n",
    "df[\"Model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.countplot(y=\"Model\", data=df, order=df[\"Model\"].value_counts().index[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above charts that our dataset contains used cars from luxury as well as budget-friendly brands.\n",
    "\n",
    "We can create a new variable using this information. We will bin all our cars into the following 3 categories later:\n",
    "\n",
    "1. Budget-Friendly\n",
    "2. Mid Range\n",
    "3. Luxury Cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Car_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby([\"Brand\"])[\"Price\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is very close to our expectation (domain knowledge), in terms of brand order. The mean price of a used Lamborghini is 120 Lakhs and that of cars from other luxury brands follow in descending order.\n",
    "\n",
    "Towards the bottom end, we have the more budget-friendly brands.\n",
    "\n",
    "We can see that there is some missingness in our data. Let us come back to creating this variable once we have removed missingness from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic summary stats - Numeric variables\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "1. S.No. clearly has no interpretation here but as discussed earlier let us drop it only after having looked at the initial linear model.\n",
    "2. Kilometers_Driven values have an incredibly high range. We should check a few of the extreme values to get a sense of the data.\n",
    "3. Minimum and maximum number of seats in the car also warrant a quick check. On average, a car seems to have 5 seats, which is about right.\n",
    "4. We have used cars being sold at less than a lakh rupees and as high as 160 lakh, as we saw for Lamborghini earlier. We might have to drop some of these outliers to build a robust model.\n",
    "5. The minimum mileage being 0 is also concerning, we'll have to check what is going on.\n",
    "6. Engine and Power mean and median values are not very different. Only someone with more domain knowledge would be able to comment further on these attributes.\n",
    "7. The new price range seems right. We have both budget-friendly Maruti cars and Lamborghinis in our stock. Mean being twice that of the median suggests that there are only a few very high range brands, which again makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking extreme values in Kilometers_Driven\n",
    "df.sort_values(by=[\"Kilometers_Driven\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the first row here is a data entry error. A car manufactured as recently as 2017 having been driven 6500000 km is almost impossible.\n",
    "\n",
    "The other observations that follow are also on a higher end. There is a good chance that these are outliers. We'll look at this further while doing the univariate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking extreme values in Kilometers_Driven\n",
    "df.sort_values(by=[\"Kilometers_Driven\"], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the columns `Year`, `New_Price`, and `Price`, these entries seem feasible.\n",
    "\n",
    "1000 might be the default value in this case. Quite a few cars having driven exactly 1000 km is suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking extreme values in Seats\n",
    "df.sort_values(by=[\"Seats\"], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audi A4 having 0 seats is clearly a data entry error. This column warrants some outlier treatment, or we can treat Seats = 0 as a missing value. Overall, there doesn't seem not much to be concerned about here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check if we have a similar car in our dataset.\n",
    "df[df[\"Name\"].str.startswith(\"Audi A4\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like an Audi A4 typically has 5 seats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us replace #seats in row index 3999 form 0 to 5\n",
    "df.loc[3999, \"Seats\"] = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking extreme values in Seats\n",
    "df.sort_values(by=[\"Seats\"], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, a Toyota Qualis has 10 seats and so does a Tata Sumo. We don't see any data entry error here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking extreme values in Mileage - km_per_unit_fuel\n",
    "df.sort_values(by=[\"km_per_unit_fuel\"], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to treat Mileage = 0 as missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking extreme values in Mileage - km_per_unit_fuel\n",
    "df.sort_values(by=[\"km_per_unit_fuel\"], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maruti Wagon R and Maruti Alto CNG versions are budget-friendly cars with high mileage, so these data points are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at value counts for non-numeric features\n",
    "\n",
    "num_to_display = 10  # defining this up here so it's easy to change later\n",
    "for colname in df.dtypes[df.dtypes == \"object\"].index:\n",
    "    val_counts = df[colname].value_counts(dropna=False)  # will also show the NA counts\n",
    "    print(val_counts[:num_to_display])\n",
    "    if len(val_counts) > num_to_display:\n",
    "        print(f\"Only displaying first {num_to_display} of {len(val_counts)} values.\")\n",
    "    print(\"\\n\\n\")  # just for more space in between"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't dropped the original columns that we processed, we have a few redundant outputs here.\n",
    "\n",
    "We had checked cars of different `Fuel_Type` earlier, but we did not encounter the 2 electric cars. Let us check why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Fuel_Type\"] == \"Electric\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mileage values for these cars are NaN, that is why we did not encounter these earlier with groupby.\n",
    "\n",
    "Electric cars are very new in the market and very rare in our dataset. We can consider dropping these two observations if they turn out to be outliers later. There is a good chance that we will not be able to create a good price prediction model for electric cars, with the currently available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Price for 6247 entries is missing. We need to explore if we can impute these or we should drop this column altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Before we start looking at the individual distributions and interactions, let's quickly check the missingness in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 Electric car variants don't have entries for Mileage.\n",
    "* Engine displacement information of 46 observations is missing and the maximum power of 175 entries is missing.\n",
    "* Information about the number of seats is not available for 53 entries.\n",
    "* New price, as we saw earlier, has a huge missing count. We'll have to see if there is a pattern here.\n",
    "* Price is also missing for 1234 entries. Since price is the response variable that we want to predict, we will have to drop these rows when we actually build a model. These rows will not be able to help us in modeling or model evaluation. But while we are analyzing the distributions and doing missing value imputations, we will keep using information from these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the redundant columns.\n",
    "df.drop(\n",
    "    columns=[\"Mileage\", \"mileage_unit\", \"Engine\", \"Power\", \"New_Price\"], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "\n",
    "### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"Price\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a highly skewed distribution. Let us use log transformation on this column to see if that helps normalize the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df[\"Price\"]), kde=True)\n",
    "plt.xlabel(\"Log(price)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the log transformation has definitely helped in reducing the skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column with the transformed variable.\n",
    "df[\"price_log\"] = np.log(df[\"Price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price vs Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.boxplot(x=\"Location\", y=\"Price\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price of used cars has a large IQR in Coimbatore and Bangalore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kilometers_Driven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"Kilometers_Driven\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation\n",
    "sns.histplot(np.log(df[\"Kilometers_Driven\"]), kde=True)\n",
    "plt.xlabel(\"Log(Kilometers_Driven)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation has reduced the extreme skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"kilometers_driven_log\"] = np.log(df[\"Kilometers_Driven\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df, hue=\"Fuel_Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming into these plots gives us a lot of information.\n",
    "\n",
    "* Contrary to intuition, Kilometers Driven does not seem to have a relationship with the price.\n",
    "* Price has a positive relationship with Year. Newer the car, the higher the price.\n",
    "* S.No. does not capture any information that we were hoping for. The temporal element of variation is captured in the year column.\n",
    "* 2 seater cars are all luxury variants. Cars with 8-10 seats are exclusively mid to high range.\n",
    "* Mileage does not seem to show much relationship with the price of used cars.\n",
    "* Engine displacement and Power of the car have a positive relationship with the price.\n",
    "* New Price and Used Car Price are also positively correlated, which is expected.\n",
    "* Kilometers Driven has a peculiar relationship with the Year variable. Generally, the newer the car lesser the distance it has traveled, but this is not always true.\n",
    "* CNG cars are conspicuous outliers when it comes to Mileage. The mileage of these cars is very high.\n",
    "* The mileage and power of newer cars are increasing owing to advancements in technology.\n",
    "* Mileage has a negative correlation with engine displacement and power. More powerful the engine, the more fuel it consumes in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Power and engine are important predictors of price\n",
    "* We will have to work on imputing New Price missing values because this is a very important feature in predicting used car price accurately "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's check again for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at a few rows where seats is missing\n",
    "df[df[\"Seats\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll impute these missing values one-by-one by taking the median number of seats for the particular car using the Brand and Model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Brand\", \"Model\"], as_index=False)[\"Seats\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing Seats\n",
    "df[\"Seats\"] = df.groupby([\"Brand\", \"Model\"])[\"Seats\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'Seats'\n",
    "df[df[\"Seats\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maruti Estilo can accomodate 5\n",
    "df[\"Seats\"] = df[\"Seats\"].fillna(5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a similar method to fill missing values for engine, power, and new price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"engine_num\"] = df.groupby([\"Brand\", \"Model\"])[\"engine_num\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "df[\"power_num\"] = df.groupby([\"Brand\", \"Model\"])[\"power_num\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "df[\"new_price_num\"] = df.groupby([\"Brand\", \"Model\"])[\"new_price_num\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are still some missing values in power, mileage and new_price_num.\n",
    "* There are a few car brands and models in our dataset that do not contain the new price information at all.\n",
    "* We'll impute these missing values one-by-one by taking the median number of seats for the particular car using the Brand name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = [\"power_num\", \"km_per_unit_fuel\", \"new_price_num\"]\n",
    "\n",
    "for ii in cols1:\n",
    "    df[ii] = df.groupby([\"Brand\"])[ii].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are still some missing values in power and new_price_num.\n",
    "* We'll have to estimate the new price using median of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = [\"power_num\", \"km_per_unit_fuel\", \"new_price_num\"]\n",
    "\n",
    "for ii in cols1:\n",
    "    df[ii] = df[ii].fillna(df[ii].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the redundant columns\n",
    "df.drop(columns=[\"Kilometers_Driven\", \"Name\", \"S.No.\"], inplace=True)\n",
    "\n",
    "# dropping the rows where 'Price' == NaN, and proceed to modeling\n",
    "df = df[df[\"Price\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Building\n",
    "\n",
    "1. We want to predict the price, so we will use the normalized version 'price_log' for modeling.\n",
    "2. Before we proceed to build a model, we'll have to encode categorical features. We will drop categorical features like Name.\n",
    "3. We'll split the data into train and test to be able to evaluate the model that we build on the train data.\n",
    "4. We will build a Linear Regression model using the train data and evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_vars = df.drop([\"Price\", \"price_log\"], axis=1)\n",
    "dep_var = df[\"price_log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_vars(x):\n",
    "    x = pd.get_dummies(\n",
    "        x,\n",
    "        columns=x.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),\n",
    "        drop_first=True,\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "ind_vars_num = encode_cat_vars(ind_vars)\n",
    "ind_vars_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    ind_vars_num, dep_var, test_size=0.3, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in train data =\", x_train.shape[0])\n",
    "print(\"Number of rows in train data =\", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()\n",
    "lin_reg_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the coefficients and intercept of the model\n",
    "\n",
    "coef_df = pd.DataFrame(\n",
    "    np.append(lin_reg_model.coef_, lin_reg_model.intercept_),\n",
    "    index=x_train.columns.tolist() + [\"Intercept\"],\n",
    "    columns=[\"Coefficients\"],\n",
    ")\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us check the model performance on training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R^2\n",
    "def adj_r2(ind_vars, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = ind_vars.shape[0]\n",
    "    k = ind_vars.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# Model performance check\n",
    "def model_perf(model, inp, out):\n",
    "\n",
    "    y_pred = np.exp(model.predict(inp))\n",
    "    y_act = np.exp(out)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_act, y_pred)),\n",
    "            \"MAE\": mean_absolute_error(y_act, y_pred),\n",
    "            \"R^2\": r2_score(y_act, y_pred),\n",
    "            \"Adjusted R^2\": adj_r2(inp, y_act, y_pred),\n",
    "        },\n",
    "        index=[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model performance on train set\n",
    "print(\"Training Performance:\")\n",
    "model_perf(lin_reg_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both the R-squared and Adjusted R squared of our model are very high. This is a clear indication that we have been able to create a very good model that is able to explain variance in the price of used cars up to 94%.\n",
    "* The model is not an underfitting model.\n",
    "* Let us do a quick performance check on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model performance on test set\n",
    "print(\"Test Performance:\")\n",
    "model_perf(lin_reg_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model can explain more than 91% of the variation in the test data, which is very good.\n",
    "* Root Mean Squared Error of train and test data are close, which indicates that our model is not overfitting the train data.\n",
    "* Mean Absolute Error indicates that our current model is able to predict used car prices within a mean error of 1.3 lakhs on test data.\n",
    "* The units of both RMSE and MAE are the same - Lakhs in this case. But RMSE is greater than MAE because it penalizes the outliers more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrEXEiJEqH-0"
   },
   "source": [
    "## Forward Feature Selection\n",
    "\n",
    "Let us try using forward feature selection to check if we can get a good model performance using lesser number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lAz8-EMqH_E",
    "outputId": "01e5c5aa-6ae0-4a01-bbf0-6ed48d8f919f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Build step forward feature selection\n",
    "sfs = SFS(\n",
    "    reg,\n",
    "    k_features=x_train.shape[1],\n",
    "    forward=True,  # k_features denotes \"Number of features to select\"\n",
    "    floating=False,\n",
    "    scoring=\"r2\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1,  # this will ensure all CPU cores are being used for computation\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "# Perform SFFS\n",
    "sfs = sfs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot the performance with addition of each feature\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "fig1 = plot_sfs(sfs.get_metric_dict(), kind=\"std_err\", figsize=(15, 5))\n",
    "plt.title(\"Sequential Forward Selection (w. StdErr)\")\n",
    "plt.xticks(\n",
    "    np.arange(0, 264, 10), np.arange(0, 264, 10), rotation=90\n",
    ")  # to make the tick marks readable\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model performance appears to have become constant somewhere around 118 features and then starts falling around 239 features.\n",
    "* The improvement in performance from 85 to 118 features is not that high either.\n",
    "* We will take 85 as the number of features to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysSkaRN9qH_M",
    "outputId": "44accd16-22a9-414b-c612-8d3665759225"
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "\n",
    "# # Build step forward feature selection\n",
    "sfs = SFS(\n",
    "    reg,\n",
    "    k_features=85,\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring=\"r2\",\n",
    "    verbose=2,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Perform SFFS\n",
    "sfs = sfs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRthzjjOqH_O",
    "outputId": "c240b1eb-b66d-4fcc-bc95-f6eef482cec0"
   },
   "outputs": [],
   "source": [
    "# Which features are important?\n",
    "feat_cols = list(sfs.k_feature_idx_)\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7U3DS_2qH_Q",
    "outputId": "71f329fb-e87a-4f09-c4a8-1065b096bebe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.columns[feat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpUjOgOSqH_U"
   },
   "source": [
    "**Now we will fit a sklearn model using these features only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQTigbMLqH_U"
   },
   "outputs": [],
   "source": [
    "x_train2 = x_train[x_train.columns[feat_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzRo4ycBqH_Y"
   },
   "outputs": [],
   "source": [
    "# Creating new x_test with the same 20 variables that we selected for x_train\n",
    "x_test2 = x_test[x_train2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsGeSdCYqH_b",
    "outputId": "0a88d676-f7d3-48be-ebb3-13fc7af2d13e"
   },
   "outputs": [],
   "source": [
    "# Fitting linear model\n",
    "lin_reg_model2 = LinearRegression()\n",
    "lin_reg_model2.fit(x_train2, y_train)\n",
    "\n",
    "# let us check the coefficients and intercept of the model\n",
    "\n",
    "coef_df = pd.DataFrame(\n",
    "    np.append(lin_reg_model2.coef_, lin_reg_model2.intercept_.flatten()),\n",
    "    index=x_train2.columns.tolist() + [\"Intercept\"],\n",
    "    columns=[\"Coefficients\"],\n",
    ")\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance on train set\n",
    "print(\"Training Performance:\")\n",
    "model_perf(lin_reg_model2, x_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance on test set\n",
    "print(\"Test Performance:\")\n",
    "model_perf(lin_reg_model2, x_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have been able to explain more than 92% of the variation in the test data using less than one-third the number of features used in the previous model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "1. With our linear regression model, we have been able to capture more than 92% of the variation in the test data.\n",
    "\n",
    "\n",
    "2. Mean Absolute Error indicates that our current model is able to predict used car prices within a mean error of approx. 1.4 lakhs on the test data.\n",
    "\n",
    "\n",
    "3. Factors like the year of manufacture, numbers of seats, maximum power of the engine, few locations, few brands, etc. tend to increase the price of the used car.\n",
    "\n",
    "\n",
    "4. Factors like distance covered in unit fuel, log of the number of kilometers drive, few locations, few brands, etc. tend to decrease the price of the used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Insights and Recommendations\n",
    "\n",
    "* Cars with a lesser number of kilometers driven should be preferred. \n",
    "\n",
    "\n",
    "* Some markets tend to have higher prices. Cars4U should focus more on these markets, and set up offices in these areas if needed.\n",
    "\n",
    "\n",
    "* We will have to analyze the cost side of things before we can talk about profitability in the business. We should gather data regarding that.\n",
    "\n",
    "\n",
    "* The next step post that would be to cluster different sets of data and see if we should make multiple models for different locations/car types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-on: Analysing predictions where we were way off the mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the rows from original data frame df where indexes are same as the training data\n",
    "original_df = df[df.index.isin(x_train2.index.values)].copy()\n",
    "\n",
    "# Extracting predicted values and residuals from the final model\n",
    "fitted_values = lin_reg_model2.predict(x_train2)\n",
    "residuals = fitted_values - y_train\n",
    "\n",
    "# Add new columns for predicted values\n",
    "original_df[\"Predicted price_log \"] = fitted_values\n",
    "original_df[\"Predicted Price\"] = np.exp(fitted_values)\n",
    "original_df[\"residuals\"] = residuals\n",
    "original_df[\"Abs_residuals\"] = np.exp(residuals)\n",
    "original_df[\"Difference in Lakhs\"] = np.abs(\n",
    "    original_df[\"Price\"] - original_df[\"Predicted Price\"]\n",
    ")\n",
    "\n",
    "# Let us look at the top 50 predictions where our model made highest extimation errors (on train data)\n",
    "original_df.sort_values(by=[\"Difference in Lakhs\"], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A 2017 Land Rover, whose new model sells at 230 Lakhs and the used version sold at 160 Lakhs was predicted to be sold at < 3 Lakhs. It is not apparent after looking at numerical predictors, why our model predicted such low value here. This could be because many other land rovers in our data seems to have sold at lower prices.\n",
    "* Another entry in the list here is a Lamborghini Gallardo that was sold at 120 Lakhs but our model predicted the price around 3 lakhs. This is a huge error by the model. However, there might be a data entry error here as the price of a new Gallardo is set at around 11 Lakhs, which is less than the selling price of a used Gallardo.\n",
    "* There are a few instances where the model predicts lesser than the actual selling price. These could be a cause for concern. The model predicting lesser than potential selling price is not good for business.\n",
    "\n",
    "Let us quickly visualise some of these observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=original_df,\n",
    "    x=\"Difference in Lakhs\",\n",
    "    y=\"Price\",\n",
    "    hue=original_df[\"Fuel_Type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicts that resale value of diesel cars is higher compared to petrol cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-on: Advanced Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Mileage\n",
    "\n",
    "We have car mileage in two units as per the data dictionary - kmpl and km/kg.\n",
    "\n",
    "After a quick research on the internet, it is clear that these 2 units are used for cars of 2 different fuel types.\n",
    "\n",
    "* kmpl (kilometers per litre) is used for petrol and diesel cars.\n",
    "* km/kg (kilometers per kg) is used for CNG and LPG-based engines.\n",
    "\n",
    "We have the variable `Fuel_type` in our data. Let us check if these observations hold true in our data also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 new columns after splitting the mileage values.\n",
    "km_per_unit_fuel = []\n",
    "mileage_unit = []\n",
    "\n",
    "for observation in df[\"Mileage\"]:\n",
    "    if isinstance(observation, str):\n",
    "        if (\n",
    "            observation.split(\" \")[0]\n",
    "            .replace(\".\", \"\", 1)\n",
    "            .isdigit()  # first element should be numeric\n",
    "            and \" \" in observation  # space between numeric and unit\n",
    "            and (\n",
    "                observation.split(\" \")[1]\n",
    "                == \"kmpl\"  # units are limited to \"kmpl\" and \"km/kg\"\n",
    "                or observation.split(\" \")[1] == \"km/kg\"\n",
    "            )\n",
    "        ):\n",
    "            km_per_unit_fuel.append(float(observation.split(\" \")[0]))\n",
    "            mileage_unit.append(observation.split(\" \")[1])\n",
    "        else:\n",
    "            # To detect if there are any observations in the column that do not follow\n",
    "            # the expected format [number + ' ' + 'kmpl' or 'km/kg']\n",
    "            print(\n",
    "                \"The data needs further processing. All values are not similar \",\n",
    "                observation,\n",
    "            )\n",
    "    else:\n",
    "        # If there are any missing values in the mileage column,\n",
    "        # we add corresponding missing values to the 2 new columns\n",
    "        km_per_unit_fuel.append(np.nan)\n",
    "        mileage_unit.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No print output from the function above. The values are all in the expected format or NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new columns to the data\n",
    "\n",
    "df[\"km_per_unit_fuel\"] = km_per_unit_fuel\n",
    "df[\"mileage_unit\"] = mileage_unit\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check if the units correspond to the fuel types\n",
    "df.groupby(by=[\"Fuel_Type\", \"mileage_unit\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, km/kg is for CNG/LPG cars and kmpl is for Petrol and Diesel cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Engine \n",
    "\n",
    "As per the data dictionary, the `Engine` column indicates the displacement volume of the engine in CC.\n",
    "\n",
    "We will make sure that all the observations follow the same format - [numeric + \" \" + \"CC\"] and create a new numeric column from this column.\n",
    "\n",
    "This time, let's use a regex to make all the necessary checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re module provides support for regular expressions\n",
    "import re\n",
    "\n",
    "# Create a new column after splitting the engine values.\n",
    "engine_num = []\n",
    "\n",
    "# Regex for numeric + \" \" + \"CC\"  format\n",
    "regex_engine = \"^\\d+(\\.\\d+)? CC$\"\n",
    "\n",
    "for observation in df[\"Engine\"]:\n",
    "    if isinstance(observation, str):\n",
    "        if re.match(regex_engine, observation):\n",
    "            engine_num.append(float(observation.split(\" \")[0]))\n",
    "        else:\n",
    "            # To detect if there are any observations in the column that do not follow [numeric + \" \" + \"CC\"]  format\n",
    "            print(\n",
    "                \"The data needs furthur processing. All values are not similar \",\n",
    "                observation,\n",
    "            )\n",
    "    else:\n",
    "        # If there are any missing values in the engine column, we add missing values to the new column\n",
    "        engine_num.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No print output from the function above. The values are all in the same format - [numeric + \" \" + \"CC\"] or NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the new column to the data\n",
    "\n",
    "df[\"engine_num\"] = engine_num\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Power \n",
    "\n",
    "As per the data dictionary, the `Power` column indicates the maximum power of the engine in bhp.\n",
    "\n",
    "We will make sure that all the observations follow the same format - [numeric + \" \" + \"bhp\"] and create a new numeric column from this column, like we did for the `Engine` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column after splitting the power values.\n",
    "power_num = []\n",
    "\n",
    "# Regex for numeric + \" \" + \"bhp\"  format\n",
    "regex_power = \"^\\d+(\\.\\d+)? bhp$\"\n",
    "\n",
    "for observation in df[\"Power\"]:\n",
    "    if isinstance(observation, str):\n",
    "        if re.match(regex_power, observation):\n",
    "            power_num.append(float(observation.split(\" \")[0]))\n",
    "        else:\n",
    "            # To detect if there are any observations in the column that do not follow [numeric + \" \" + \"bhp\"]  format\n",
    "            # that we see in the sample output\n",
    "            print(\n",
    "                \"The data needs furthur processing. All values are not similar \",\n",
    "                observation,\n",
    "            )\n",
    "    else:\n",
    "        # If there are any missing values in the power column, we add missing values to the new column\n",
    "        power_num.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some Null values in power column exist as 'null bhp' string.\n",
    "Let us replace these with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_num = []\n",
    "\n",
    "for observation in df[\"Power\"]:\n",
    "    if isinstance(observation, str):\n",
    "        if re.match(regex_power, observation):\n",
    "            power_num.append(float(observation.split(\" \")[0]))\n",
    "        else:\n",
    "            power_num.append(np.nan)\n",
    "    else:\n",
    "        # If there are any missing values in the power column, we add missing values to the new column\n",
    "        power_num.append(np.nan)\n",
    "\n",
    "# Add the new column to the data\n",
    "df[\"power_num\"] = power_num\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. New_Price \n",
    "\n",
    "We know that `New_Price` is the price of a new car of the same model in INR Lakhs.(1 Lakh INR = 100,000 INR)\n",
    "\n",
    "This column clearly has a lot of missing values. We will impute the missing values later. For now we will only extract the numeric values from this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column after splitting the New_Price values.\n",
    "new_price_num = []\n",
    "\n",
    "# Regex for numeric + \" \" + \"Lakh\"  format\n",
    "regex_power = \"^\\d+(\\.\\d+)? Lakh$\"\n",
    "\n",
    "for observation in df[\"New_Price\"]:\n",
    "    if isinstance(observation, str):\n",
    "        if re.match(regex_power, observation):\n",
    "            new_price_num.append(float(observation.split(\" \")[0]))\n",
    "        else:\n",
    "            # To detect if there are any observations in the column that do not follow [numeric + \" \" + \"Lakh\"]  format\n",
    "            # that we see in the sample output\n",
    "            print(\n",
    "                \"The data needs furthur processing. All values are not similar \",\n",
    "                observation,\n",
    "            )\n",
    "    else:\n",
    "        # If there are any missing values in the New_Price column, we add missing values to the new column\n",
    "        new_price_num.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not all values are in Lakhs.\n",
    "\n",
    "* There are a few observations that are in Crores as well\n",
    "\n",
    "* Let us convert these to lakhs using the conversion rate 1 Cr = 100 Lakhs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_price_num = []\n",
    "\n",
    "for observation in df[\"New_Price\"]:\n",
    "    if isinstance(observation, str):\n",
    "        if re.match(regex_power, observation):\n",
    "            new_price_num.append(float(observation.split(\" \")[0]))\n",
    "        else:\n",
    "            # Converting values in Crore to lakhs\n",
    "            new_price_num.append(float(observation.split(\" \")[0]) * 100)\n",
    "    else:\n",
    "        # If there are any missing values in the New_Price column, we add missing values to the new column\n",
    "        new_price_num.append(np.nan)\n",
    "\n",
    "# Add the new column to the data\n",
    "df[\"new_price_num\"] = new_price_num\n",
    "\n",
    "# Checking the new dataframe\n",
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
