import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Added OneHotEncoder here
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import psycopg2
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

def preprocess_features(X, handle_method='impute'):
    """
    Preprocess features by handling missing values and scaling
    
    Parameters:
    X : DataFrame
    handle_method : str, 'impute' or 'drop'
        Method to handle missing values
    """
    print(f"\nInitial shape: {X.shape}")
    
    if handle_method == 'drop':
        # Drop rows with any missing values
        X = X.dropna()
        print(f"Shape after dropping NaN rows: {X.shape}")
        
    # Check for infinite values
    X = X.replace([np.inf, -np.inf], np.nan)
    
    # Get numeric and categorical columns
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X.select_dtypes(include=['object', 'category']).columns
    
    # Create preprocessing pipelines for numeric and categorical data
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'))
    ])
    
    # Combine transformers
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features) if len(categorical_features) > 0 else None
        ],
        remainder='drop'
    )
    
    # Remove None transformers
    preprocessor.transformers = [t for t in preprocessor.transformers if t is not None]
    
    # Fit and transform the data
    X_processed = preprocessor.fit_transform(X)
    
    # Get feature names after preprocessing
    numeric_cols = numeric_features.tolist()
    if len(categorical_features) > 0:
        categorical_cols = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names(categorical_features)
        feature_names = numeric_cols + categorical_cols.tolist()
    else:
        feature_names = numeric_cols
    
    print(f"Final shape after preprocessing: {X_processed.shape}")
    
    return X_processed, preprocessor, feature_names

def train_pca_model(X, n_components=0.95):
    """Train PCA model with preprocessed features"""
    # Apply PCA
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)
    
    # Get explained variance ratio
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)
    
    print(f"Number of components selected: {pca.n_components_}")
    print(f"Explained variance ratio: {cumulative_variance[-1]:.4f}")
    
    return pca, X_pca, cumulative_variance

def analyze_missing_data(df):
    """Analyze missing data in DataFrame"""
    missing_stats = pd.DataFrame({
        'missing_count': df.isnull().sum(),
        'missing_percentage': (df.isnull().sum() / len(df) * 100).round(2)
    })
    missing_stats = missing_stats[missing_stats['missing_count'] > 0].sort_values('missing_count', ascending=False)
    return missing_stats

def main(app_name, db_params, excel_path):
    """Main function with enhanced error handling and data preprocessing"""
    try:
        # Load and prepare data
        # ... (previous data loading code remains the same) ...
        
        # Create feature matrix
        X, y = create_feature_matrix(alerts_df, outage_df)
        
        # Analyze missing data
        missing_stats = analyze_missing_data(X)
        print("\nMissing Data Analysis:")
        print(missing_stats)
        
        # Preprocess features
        X_processed, preprocessor, feature_names = preprocess_features(X)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Apply PCA
        pca, X_train_pca, cumulative_variance = train_pca_model(X_train)
        
        # Transform test data
        X_test_pca = pca.transform(X_test)
        
        # Train model
        model = LogisticRegression(class_weight='balanced', random_state=42)
        model.fit(X_train_pca, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test_pca)
        y_pred_proba = model.predict_proba(X_test_pca)[:, 1]
        
        # Calculate metrics
        classification_rep = classification_report(y_test, y_pred)
        conf_matrix = confusion_matrix(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        # Analyze feature importance
        feature_importance = analyze_feature_importance(
            model, pca, feature_names, preprocessor, top_n=20
        )
        
        # Generate report
        generate_enhanced_report(
            app_name=app_name,
            data_shape=X.shape,
            model_results={
                'classification_report': classification_rep,
                'confusion_matrix': conf_matrix,
                'roc_auc': roc_auc,
                'y_test': y_test
            },
            feature_importance=feature_importance,
            missing_stats=missing_stats
        )
        
    except Exception as e:
        print(f"Error in analysis pipeline: {str(e)}")
        import traceback
        print(traceback.format_exc())

def analyze_feature_importance(model, pca, feature_names, preprocessor, top_n=20):
    """Enhanced feature importance analysis"""
    # Get PCA components importance
    feature_importance_pca = np.abs(pca.components_).sum(axis=0)
    
    # Calculate feature importance considering PCA transformation
    feature_importance = np.abs(np.dot(model.coef_, pca.components_))
    
    # Create feature importance dictionary
    importance_dict = dict(zip(feature_names, feature_importance[0]))
    
    # Sort features by importance
    sorted_features = dict(sorted(
        importance_dict.items(),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:top_n])
    
    return {
        'top_features': sorted_features,
        'cumulative_variance': np.cumsum(pca.explained_variance_ratio_),
        'pca_importance': feature_importance_pca
    }

def generate_enhanced_report(app_name, data_shape, model_results, 
                           feature_importance, missing_stats):
    """Generate enhanced HTML report with missing data analysis"""
    # Update the HTML template to include missing data analysis
    report_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Enhanced Outage Prediction Analysis - {app_name}</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
            .section {{ margin-bottom: 30px; padding: 20px; border: 1px solid #ddd; border-radius: 5px; }}
            .metric {{ display: inline-block; margin: 10px; padding: 15px; background-color: #f8f9fa; border-radius: 5px; }}
            .important {{ color: #d73a49; }}
            table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
            .feature-importance {{ display: flex; align-items: center; margin: 5px 0; }}
            .feature-bar {{ height: 20px; background-color: #0366d6; margin-left: 10px; }}
        </style>
    </head>
    <body>
        <h1>Enhanced Outage Prediction Analysis - {app_name}</h1>
        
        <div class="section">
            <h2>Dataset Overview</h2>
            <div class="metric">
                <strong>Total Samples:</strong> {n_samples}
            </div>
            <div class="metric">
                <strong>Features:</strong> {n_features}
            </div>
            <div class="metric">
                <strong>Outage Events:</strong> {n_outages}
            </div>
        </div>
        
        <div class="section">
            <h2>Missing Data Analysis</h2>
            <table>
                <tr>
                    <th>Feature</th>
                    <th>Missing Count</th>
                    <th>Missing Percentage</th>
                </tr>
                {missing_data_rows}
            </table>
        </div>
        
        <div class="section">
            <h2>Model Performance</h2>
            <div class="metric">
                <strong>ROC-AUC Score:</strong> {roc_auc:.4f}
            </div>
            <h3>Classification Report</h3>
            <pre>{classification_report}</pre>
            
            <h3>Confusion Matrix</h3>
            <pre>{confusion_matrix}</pre>
        </div>
        
        <div class="section">
            <h2>Feature Importance Analysis</h2>
            <p>Top {top_n} most important features:</p>
            <div id="feature-importance-viz">
                {feature_importance_bars}
            </div>
        </div>
        
        <div class="section">
            <h2>PCA Analysis</h2>
            <p>Cumulative explained variance:</p>
            <pre>{cumulative_variance}</pre>
        </div>
    </body>
    </html>
    """
    
    # Create missing data rows
    missing_data_rows = ""
    for feature, row in missing_stats.iterrows():
        missing_data_rows += f"""
        <tr>
            <td>{feature}</td>
            <td>{int(row['missing_count'])}</td>
            <td>{row['missing_percentage']}%</td>
        </tr>
        """
    
    # Create feature importance visualization
    feature_bars = ""
    max_importance = max(abs(v) for v in feature_importance['top_features'].values())
    for feature, importance in feature_importance['top_features'].items():
        width_percent = (abs(importance) / max_importance) * 100
        feature_bars += f"""
        <div class="feature-importance">
            <span>{feature}</span>
            <div class="feature-bar" style="width: {width_percent}%"></div>
            <span>{importance:.4f}</span>
        </div>
        """
    
    # Generate report content
    report_content = report_template.format(
        app_name=app_name,
        n_samples=data_shape[0],
        n_features=data_shape[1],
        n_outages=sum(model_results['y_test']),
        missing_data_rows=missing_data_rows,
        roc_auc=model_results['roc_auc'],
        classification_report=model_results['classification_report'],
        confusion_matrix=model_results['confusion_matrix'],
        feature_importance_bars=feature_bars,
        cumulative_variance=feature_importance['cumulative_variance'],
        top_n=len(feature_importance['top_features'])
    )
    
    # Save report
    filename = f"enhanced_outage_prediction_{app_name}_{datetime.now().strftime('%Y%m%d')}.html"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"\nAnalysis report generated: {filename}")

# Example usage:
if __name__ == "__main__":
    from sklearn.preprocessing import OneHotEncoder  # Add this import at the top
    
    # Configuration
    db_params = {
        "dbname": "your_database",
        "user": "your_username",
        "password": "your_password",
        "host": "your_host",
        "port": "your_port"
    }
    
    excel_path = "path_to_your_outage_data.xlsx"
    app_name = "YOUR_APP_NAME"
    
    main(app_name, db_params, excel_path)
